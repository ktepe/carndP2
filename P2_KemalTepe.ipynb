{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Traffic Sign Classifier\n",
    "\n",
    "## Kemal Tepe, ketepe@gmail.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setup important imports and libs \n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.contrib.layers import flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# debug printing sections, 1 if they need printing\n",
    "debug_prt=1\n",
    "image_show=1\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#utility function from RGB to Grayscale\n",
    "#\n",
    "def rgb2gray(input_data):\n",
    "    #below is weigths of RGB to Gray conversion from the literature\n",
    "    rgbmix=[0.3, 0.585, 0.115]\n",
    "    #\n",
    "    #below is equal weigths but above did worked better\n",
    "    #rgbmix=[0.333, 0.333, 0.334]\n",
    "    for i in range(len(input_data)):\n",
    "        image2=input_data[i,:,:,:]\n",
    "        input_data[i,:,:,0]=np.dot(image2[...,:3], rgbmix)\n",
    "\n",
    "    return input_data[:,:,:,0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#my convolutional NN architecture. \n",
    "#it is derived from Lenet 5 architecture, \n",
    "#but optimized by using number of hidden nodes for the traffic sign clasification \n",
    "#\n",
    "def LeNet(x):    \n",
    "    # Arguments used for tf.truncated_normal, randomly defines variables for the weights and biases for each layer\n",
    "    mu = 0\n",
    "    sigma = 0.03 # hyperparameter for noise\n",
    "    #set up the number of nodes between the layers\n",
    "    w1nodes=48 # number of conv hidden nodes in the conv NN in the layer 1\n",
    "    w2nodes=96 # number of conv hidden nodes in the conv NN in the layer 2 \n",
    "    w3nodes=5*5*w2nodes #number of fully connected nodes in layer 3 and flattening \n",
    "    w4nodes=140 #number of fully connected nodes in layer 4\n",
    "    w5nodes=96 #number of fully connected nodes in Layer 5\n",
    "    outputnodes=43 #number of output nodes, since there are 43 signs in the data set.\n",
    "    \n",
    "    # SOLUTION: Layer 1: Convolutional. Input = 32x32x1. Output = 28x28x(w1nodes).\n",
    "    conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 1, w1nodes), mean = mu, stddev = sigma))\n",
    "    conv1_b = tf.Variable(tf.zeros(w1nodes))\n",
    "    conv1   = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n",
    "\n",
    "    # SOLUTION: Activation.\n",
    "    #instead of RELU, i used ELU non-linear function\n",
    "    conv1 = tf.nn.elu(conv1)\n",
    "\n",
    "    # SOLUTION: Pooling. Input = 28x28x(w1nodes). Output = 14x14x(w1nodes).\n",
    "    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    # SOLUTION: Layer 2: Convolutional. Output = 10x10x(w2nodes).\n",
    "    conv2_W = tf.Variable(tf.truncated_normal(shape=(5, 5, w1nodes, w2nodes), mean = mu, stddev = sigma))\n",
    "    conv2_b = tf.Variable(tf.zeros(w2nodes))\n",
    "    conv2   = tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding='VALID') + conv2_b\n",
    "    \n",
    "    # SOLUTION: Activation.\n",
    "    conv2 = tf.nn.elu(conv2)\n",
    "\n",
    "    # SOLUTION: Pooling. Input = 10x10x1(w2nodes) Output = 5x5x(w2nodes).\n",
    "    conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    # SOLUTION: Flatten. Input = 5x5x(w2nodes). Output = w3nodes.\n",
    "    fc0   = flatten(conv2)\n",
    "    \n",
    "    # SOLUTION: Layer 3: Fully Connected. Input = w3nodes Output = w4nodes\n",
    "    \n",
    "    fc1_W = tf.Variable(tf.truncated_normal(shape=(w3nodes, w4nodes), mean = mu, stddev = sigma))\n",
    "    fc1_b = tf.Variable(tf.zeros(w4nodes))\n",
    "    fc1   = tf.matmul(fc0, fc1_W) + fc1_b\n",
    "    \n",
    "    # SOLUTION: Activation.\n",
    "    fc1    = tf.nn.elu(fc1)\n",
    "\n",
    "    # SOLUTION: Layer 4: Fully Connected. Input = w4nodes Output = w5nodes\n",
    "\n",
    "    fc2_W  = tf.Variable(tf.truncated_normal(shape=(w4nodes, w5nodes), mean = mu, stddev = sigma))\n",
    "    fc2_b  = tf.Variable(tf.zeros(w5nodes))\n",
    "    fc2    = tf.matmul(fc1, fc2_W) + fc2_b\n",
    "    \n",
    "    # SOLUTION: Activation.\n",
    "    fc2    = tf.nn.elu(fc2)\n",
    "\n",
    "    # SOLUTION: Layer 5: Fully Connected. Input = w5nodes Output = outputnodes.\n",
    "    \n",
    "    fc3_W  = tf.Variable(tf.truncated_normal(shape=(w5nodes, outputnodes), mean = mu, stddev = sigma))\n",
    "    fc3_b  = tf.Variable(tf.zeros(outputnodes))\n",
    "    logits = tf.matmul(fc2, fc3_W) + fc3_b\n",
    "    \n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#evaluation function\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reading the training, validation and testing data \n",
    "#stored in the subfolder \"traffic-signs-data\" folder\n",
    "training_file = './traffic-signs-data/train.p'\n",
    "validation_file='./traffic-signs-data/valid.p'\n",
    "testing_file = './traffic-signs-data/test.p'\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(validation_file, mode='rb') as f:\n",
    "    valid = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "X_train, y_train = train['features'], train['labels']\n",
    "X_valid, y_valid = valid['features'], valid['labels']\n",
    "X_test, y_test = test['features'], test['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples = 34799\n",
      "Number of testing examples = 12630\n",
      "Image data shape = (32, 32, 3)\n",
      "Number of classes = 4\n"
     ]
    }
   ],
   "source": [
    "#Number of training examples\n",
    "n_train = len(X_train)\n",
    "\n",
    "#Number of testing examples.\n",
    "n_test = len(X_test)\n",
    "\n",
    "#the shape of an traffic sign image\n",
    "image_shape = X_train[0].shape\n",
    "\n",
    "#How many unique classes/labels there are in the dataset.\n",
    "n_classes = len(train)\n",
    "\n",
    "print(\"Number of training examples =\", n_train)\n",
    "print(\"Number of testing examples =\", n_test)\n",
    "print(\"Image data shape =\", image_shape)\n",
    "print(\"Number of classes =\", n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image index 37 --image name-- Go straight or left\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFsAAABZCAYAAABR/liSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/weZAE/ZtHivKgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbb12cd7550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizations will be shown in the notebook.\n",
    "index = random.randint(0, n_train)\n",
    "image = X_train[index].squeeze()\n",
    "\n",
    "signnames=pd.DataFrame(pd.read_csv(\"signnames.csv\"))\n",
    "#in order to show these, the switch values must be 1 in the first cell.\n",
    "if debug_prt:\n",
    "    print('image index', y_train[index], '--image name--', signnames.loc[y_train[index]]['SignName'])\n",
    "\n",
    "if image_show:\n",
    "    plt.figure(figsize=(1,1))\n",
    "    plt.imshow(image, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFsAAABZCAYAAABR/liSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbae2907630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#RGB to Gray conversions\n",
    "X_train=rgb2gray(X_train)\n",
    "X_valid=rgb2gray(X_valid)\n",
    "X_test=rgb2gray(X_test)\n",
    "if image_show:\n",
    "    plt.figure(figsize=(1,1))\n",
    "    plt.imshow(image)\n",
    "    \n",
    "#shuffle image so it will get different sequence for training     \n",
    "X_train, y_train = shuffle(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate 0.001 Epoch 1 Batch Size 128\n"
     ]
    }
   ],
   "source": [
    "# setup the training parameters\n",
    "#original rate = 0.001 \n",
    "rate=0.001\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 128\n",
    "print('learning rate', rate, 'Epoch', EPOCHS, 'Batch Size', BATCH_SIZE)\n",
    "\n",
    "# setup tensors to be feed at each epoch\n",
    "x = tf.placeholder(tf.float32, (None, 32, 32, 1))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "one_hot_y = tf.one_hot(y,43)\n",
    "\n",
    "### Train your model here.\n",
    "#set up the system\n",
    "logits = LeNet(x)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=one_hot_y)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
    "training_operation = optimizer.minimize(loss_operation)\n",
    "\n",
    "#Evaluate how well the loss and accuracy of the model for a given dataset.\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "saver = tf.train.Saver()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "\n",
      "EPOCH 1 ...\n",
      "Validation Accuracy = 0.912\n",
      "\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "### Calculate and report the accuracy on the training and validation set.\n",
    "### Once a final model architecture is selected, \n",
    "### the accuracy on the test set should be calculated and reported as well.\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_train)\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    print()\n",
    "    for i in range(EPOCHS):\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "            \n",
    "        validation_accuracy = evaluate(X_valid, y_valid)\n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "        print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "        print()\n",
    "        \n",
    "    saver.save(sess, './lenet')\n",
    "    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 0.898\n"
     ]
    }
   ],
   "source": [
    "#test accucary\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
    "\n",
    "    test_accuracy = evaluate(X_test, y_test)\n",
    "    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Internet Road Sign Accuracy = 0.600\n"
     ]
    }
   ],
   "source": [
    "#internet road signs\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "image0 = mpimg.imread('roadsigns/noentry_32.jpg')\n",
    "image1 = mpimg.imread('roadsigns/nopedestrian_32.jpg')\n",
    "image2 = mpimg.imread('roadsigns/speedlimit30_32.jpg')\n",
    "image3 = mpimg.imread('roadsigns/stop_32.jpg')\n",
    "image4 = mpimg.imread('roadsigns/yield_32.jpg')\n",
    "image_labels=[17,27,1,14,13]\n",
    "\n",
    "x_image=np.zeros((5,32,32,3))\n",
    "\n",
    "x_image[0,:,:,:]=image0\n",
    "x_image[1,:,:,:]=image1\n",
    "x_image[2,:,:,:]=image2\n",
    "x_image[3,:,:,:]=image3\n",
    "x_image[4,:,:,:]=image4\n",
    "\n",
    "x_image=rgb2gray(x_image)\n",
    "\n",
    "#run with the trained session\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
    "\n",
    "    test_accuracy = evaluate(x_image, image_labels)\n",
    "    print(\"Internet Road Sign Accuracy = {:.3f}\".format(test_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:carnd-term1]",
   "language": "python",
   "name": "conda-env-carnd-term1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
